{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5513a2d",
   "metadata": {},
   "source": [
    "General imports and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e72d7a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venkata_v/envs/env_py37/lib64/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "in_notebooks_dir = (\n",
    "    (os.path.basename(os.getcwd()) == 'notebooks') and \n",
    "    (os.path.exists(os.path.join(os.path.dirname(os.getcwd()), 'src')))\n",
    ")\n",
    "\n",
    "if in_notebooks_dir:\n",
    "    os.chdir(os.path.dirname(os.getcwd()))\n",
    "\n",
    "srcdir = os.path.join('..', 'src')\n",
    "if srcdir not in sys.path:\n",
    "    sys.path.insert(0, srcdir)\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca9702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Function to patch images with patch SIZE and STRIDE variables\n",
    "\"\"\"\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import patchify\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "import glob as glob\n",
    "import os\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "STRIDE = 250\n",
    "SIZE = 300\n",
    "\n",
    "def create_patches(\n",
    "    input_paths, out_hr_path, out_lr_path,\n",
    "):\n",
    "    \n",
    "    shutil.rmtree(out_hr_path)\n",
    "    shutil.rmtree(out_lr_path)\n",
    "    os.makedirs(out_hr_path, exist_ok=True)\n",
    "    os.makedirs(out_lr_path, exist_ok=True)\n",
    "\n",
    "    all_paths = []\n",
    "    print(input_paths)\n",
    "    for input_path in input_paths:\n",
    "        all_paths.extend(glob.glob(f\"{input_path}/*\"))\n",
    "    print(f\"Creating patches for {len(all_paths)} images\")\n",
    "\n",
    "    for image_path in tqdm(all_paths, total=len(all_paths)):\n",
    "        image = Image.open(image_path)\n",
    "        image_name = image_path.split(os.path.sep)[-1].split('.')[0]\n",
    "        print(image_name)\n",
    "        w, h = image.size\n",
    "        patches = patchify.patchify(np.array(image), (SIZE, SIZE, 3), STRIDE)\n",
    "\n",
    "        counter = 0\n",
    "        for i in range(patches.shape[0]):\n",
    "            for j in range(patches.shape[1]):\n",
    "                counter += 1\n",
    "                patch = patches[i, j, 0, :, :, :]\n",
    "                patch = cv2.cvtColor(patch, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(\n",
    "                    f\"{out_hr_path}/{image_name}_{counter}.png\",\n",
    "                    patch\n",
    "                )\n",
    "\n",
    "                # Convert to bicubic and save.\n",
    "                h, w, _ = patch.shape\n",
    "                low_res_img = cv2.resize(patch, (int(w*0.5), int(h*0.5)), \n",
    "                                        interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "                # Now upscale using BICUBIC.\n",
    "                high_res_upscale = cv2.resize(low_res_img, (w, h), \n",
    "                                            interpolation=cv2.INTER_CUBIC)\n",
    "                cv2.imwrite(\n",
    "                    f\"{out_lr_path}/{image_name}_{counter}.png\",\n",
    "                    high_res_upscale\n",
    "                )\n",
    "\n",
    "create_patches(\n",
    "        ['./data/raw/CGG_data/train/gt'],\n",
    "        './data/interim/patches/hr_patches',\n",
    "        './data/interim/patches/lr_patches'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03bfa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "### utils\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "def psnr(label, outputs, max_val=1.):\n",
    "    \"\"\"\n",
    "    Compute Peak Signal to Noise Ratio (the higher the better).\n",
    "    PSNR = 20 * log10(MAXp) - 10 * log10(MSE).\n",
    "    \n",
    "    Note that the output and label pixels (when dealing with images) should\n",
    "    be normalized as the `max_val` here is 1 and not 255.\n",
    "    \"\"\"\n",
    "    label = label.cpu().detach().numpy()\n",
    "    outputs = outputs.cpu().detach().numpy()\n",
    "    diff = outputs - label\n",
    "    rmse = math.sqrt(np.mean((diff) ** 2))\n",
    "    if rmse == 0:\n",
    "        return 100\n",
    "    else:\n",
    "        PSNR = 20 * math.log10(max_val / rmse)\n",
    "        return PSNR\n",
    "\n",
    "def save_plot(train_loss, val_loss, train_psnr, val_psnr):\n",
    "    # Loss plots.\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_loss, color='orange', label='train loss')\n",
    "    plt.plot(val_loss, color='red', label='validataion loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('./data/srcnn_outputs/loss.png')\n",
    "    plt.close()\n",
    "\n",
    "    # PSNR plots.\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(train_psnr, color='green', label='train PSNR dB')\n",
    "    plt.plot(val_psnr, color='blue', label='validataion PSNR dB')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('PSNR (dB)')\n",
    "    plt.legend()\n",
    "    plt.savefig('./data/srcnn_outputs/psnr.png')\n",
    "    plt.close()\n",
    "\n",
    "def save_model_state(model):\n",
    "    # save the model to disk\n",
    "    print('Saving model...')\n",
    "    torch.save(model.state_dict(), './data/srcnn_outputs/model.pth')\n",
    "\n",
    "def save_model(epochs, model, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Function to save the trained model to disk.\n",
    "    \"\"\"\n",
    "    # Remove the last model checkpoint if present.\n",
    "    torch.save({\n",
    "                'epoch': epochs+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                }, f\"./data/srcnn_outputs/model_ckpt.pth\")\n",
    "\n",
    "def save_validation_results(outputs, epoch, batch_iter):\n",
    "    \"\"\"\n",
    "    Function to save the validation reconstructed images.\n",
    "    \"\"\"\n",
    "    save_image(\n",
    "        outputs, \n",
    "        f\"./data/srcnn_outputs/val_sr_{epoch}_{batch_iter}.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "780f7364",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train, Test split\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import src.utils.np_utils as npu\n",
    "import json\n",
    "\n",
    "X = os.listdir(os.getcwd() + '/data/interim/patches/lr_patches')\n",
    "y = os.listdir(os.getcwd() + '/data/interim/patches/hr_patches')\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=100)\n",
    "\n",
    "\n",
    "means, stds = npu.compute_stats_channel_dim(os.getcwd() + '/data/interim/patches/lr_patches/', X_train)\n",
    "\n",
    "preprocessing_json = {}\n",
    "\n",
    "preprocessing_json['means'] = means\n",
    "preprocessing_json['stds']  = stds\n",
    "preprocessing_json['train'] = X_train\n",
    "preprocessing_json['val']   = X_val\n",
    "preprocessing_json['test']  = X_test\n",
    "\n",
    "\n",
    "with open(os.getcwd() + \"/data/processed/preprocessing.json\", \"w\") as f:\n",
    "    json.dump(preprocessing_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9af3ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from torchvision import transforms\n",
    "\n",
    "with open(os.getcwd() + \"/data/processed/preprocessing.json\", 'r') as test_file:\n",
    "    preprocessing_dict = json.load(test_file)\n",
    "\n",
    "\n",
    "# check whether separate rgb_means are needed for val, test\n",
    "rgb_means = preprocessing_dict['means']\n",
    "rgb_stds = preprocessing_dict['stds']\n",
    "\n",
    "\n",
    "data_transforms = {\n",
    "    'train_input': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=rgb_means, std=rgb_stds),\n",
    "        #Pad(desired_size=(3, 107, 107)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ]),\n",
    "    'train_target': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=rgb_means, std=rgb_stds),\n",
    "        #Pad(desired_size=(3, 1070, 1070)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "    ]),\n",
    "    'val_input': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=rgb_means, std=rgb_stds),\n",
    "        #Pad(desired_size=(3, 107, 107)),\n",
    "    ]),\n",
    "    'val_target': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=rgb_means, std=rgb_stds),\n",
    "        #Pad(desired_size=(3, 1070, 1070)),\n",
    "    ]),\n",
    "    'test_input': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=rgb_means, std=rgb_stds),\n",
    "        #Pad(desired_size=(3, 107, 107)),\n",
    "    ]),\n",
    "    'test_target': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=rgb_means, std=rgb_stds),\n",
    "        #Pad(desired_size=(3, 1070, 1070)),\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c4f1ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 7\n",
      "Validation samples: 2\n",
      "Testing samples: 2\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from src.models.models import SimpleModel\n",
    "from src.data.datasets import SRDataset\n",
    "from src.data.transforms import Pad\n",
    "from src.utils.torch_utils import reverse_image_standardisation\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "\n",
    "train_dataset = SRDataset(\n",
    "    fnames = preprocessing_dict['train'],\n",
    "    img_dir = f'data/interim/patches/lr_patches',\n",
    "    target_dir = f'data/interim/patches/hr_patches',\n",
    "    transform=data_transforms['train_input'],\n",
    "    target_transform=data_transforms['train_target'],\n",
    ")\n",
    "val_dataset = SRDataset(\n",
    "    fnames = preprocessing_dict['val'],\n",
    "    img_dir = f'data/interim/patches/lr_patches',\n",
    "    target_dir = f'data/interim/patches/hr_patches',\n",
    "    transform=data_transforms['val_input'],\n",
    "    target_transform=data_transforms['val_target'],\n",
    ")\n",
    "test_dataset = SRDataset(\n",
    "    fnames = preprocessing_dict['test'],\n",
    "    img_dir = f'data/interim/patches/lr_patches',\n",
    "    target_dir = f'data/interim/patches/hr_patches',\n",
    "    transform=data_transforms['test_input'],\n",
    "    target_transform=data_transforms['test_target'],\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "print(f'Training samples: {len(train_dataloader)}')\n",
    "print(f'Validation samples: {len(val_dataloader)}')\n",
    "print(f'Testing samples: {len(test_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d447ef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model definition\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SRCNN(nn.Module):\n",
    "    '''\n",
    "    SRCNN model for pipeline testing which takes tensor images and upsamples by x times\n",
    "    Tensor images are expected as: (B x C x H x W)\n",
    "    '''\n",
    "    def __init__(self):       \n",
    "        super(SRCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=9, stride=(1, 1), padding=(2, 2))\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=1, stride=(1, 1), padding=(2, 2))\n",
    "        self.conv3 = nn.Conv2d(32, 3, kernel_size=5, stride=(1, 1), padding=(2, 2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d485639c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:28<00:00,  4.07s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train PSNR: 2.808\n",
      "Val PSNR: 7.342\n",
      "Saving model...\n",
      "Epoch 2 of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:23<00:00,  3.39s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train PSNR: 7.165\n",
      "Val PSNR: 8.236\n",
      "Saving model...\n",
      "Finished training in: 0.982 minutes\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training loop\n",
    "\"\"\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = SRCNN().to(device)\n",
    "\n",
    "epochs = 2 # Number of epochs to train the SRCNN model for.\n",
    "lr = 0.001 # Learning rate.\n",
    "\n",
    "# Optimizer.\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# Loss function. \n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_psnr = 0.0\n",
    "    for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        image_data = data[0].to(device)\n",
    "        label = data[1].to(device)\n",
    "        \n",
    "        # Zero grad the optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image_data)\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        # Backpropagation.\n",
    "        loss.backward()\n",
    "        # Update the parameters.\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add loss of each item (total items in a batch = batch size).\n",
    "        running_loss += loss.item()\n",
    "        # Calculate batch psnr (once every `batch_size` iterations).\n",
    "        batch_psnr =  psnr(label, outputs)\n",
    "        running_psnr += batch_psnr\n",
    "\n",
    "    final_loss = running_loss/len(dataloader.dataset)\n",
    "    final_psnr = running_psnr/len(dataloader)\n",
    "    return final_loss, final_psnr\n",
    "\n",
    "\n",
    "def validate(model, dataloader, epoch):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_psnr = 0.0\n",
    "    with torch.no_grad():\n",
    "        for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            image_data = data[0].to(device)\n",
    "            label = data[1].to(device)\n",
    "            \n",
    "            outputs = model(image_data)\n",
    "            loss = criterion(outputs, label)\n",
    "\n",
    "            # Add loss of each item (total items in a batch = batch size) .\n",
    "            running_loss += loss.item()\n",
    "            # Calculate batch psnr (once every `batch_size` iterations).\n",
    "            batch_psnr = psnr(label, outputs)\n",
    "            running_psnr += batch_psnr\n",
    "\n",
    "\n",
    "    final_loss = running_loss/len(dataloader.dataset)\n",
    "    final_psnr = running_psnr/len(dataloader)\n",
    "    return final_loss, final_psnr\n",
    "\n",
    "train_loss, val_loss = [], []\n",
    "train_psnr, val_psnr = [], []\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} of {epochs}\")\n",
    "    train_epoch_loss, train_epoch_psnr = train(model, train_dataloader)\n",
    "    val_epoch_loss, val_epoch_psnr = validate(model, val_dataloader, epoch+1)\n",
    "    print(f\"Train PSNR: {train_epoch_psnr:.3f}\")\n",
    "    print(f\"Val PSNR: {val_epoch_psnr:.3f}\")\n",
    "    train_loss.append(train_epoch_loss)\n",
    "    train_psnr.append(train_epoch_psnr)\n",
    "    val_loss.append(val_epoch_loss)\n",
    "    val_psnr.append(val_epoch_psnr)\n",
    "    \n",
    "    # Save model with all information every 100 epochs. Can be used \n",
    "    # resuming training.\n",
    "    if (epoch+1) % 25 == 0:\n",
    "        save_model(epoch, model, optimizer, criterion)\n",
    "    # Save the model state dictionary only every epoch. Small size, \n",
    "    # can be used for inference.\n",
    "    save_model_state(model)\n",
    "    # Save the PSNR and loss plots every epoch.\n",
    "    save_plot(train_loss, val_loss, train_psnr, val_psnr)\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Finished training in: {((end-start)/60):.3f} minutes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf5a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Inference loop\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import glob as glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def result(model,dataloader, device):\n",
    "    model.eval()\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for bi, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            counter += 1\n",
    "            hr_up_bc = data[0].to(device)\n",
    "            \n",
    "            hr_image = model(hr_up_bc)\n",
    "            save_image(hr_image, f\"./data/srcnn_outputs/results/output_hr_{counter}.png\")\n",
    "\n",
    "\n",
    "# The SRCNN dataset module.\n",
    "class SRCNNDataset(Dataset):\n",
    "    def __init__(self, image_paths):\n",
    "        self.all_image_paths = glob.glob(f\"{image_paths}/*\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.all_image_paths))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # The high resolution ground truth label.\n",
    "        label = Image.open(self.all_image_paths[index]).convert('RGB')\n",
    "        w, h = label.size[:]\n",
    "        # Convert to 2x bicubic.\n",
    "        # The low resolution input image.\n",
    "        image = label.resize((w*2, h*2), Image.BICUBIC)\n",
    "\n",
    "    \n",
    "        image = np.array(image, dtype=np.float32)\n",
    "        label = np.array(label, dtype=np.float32)\n",
    "\n",
    "        image /= 255.\n",
    "        label /= 255.\n",
    "\n",
    "        image = image.transpose([2, 0, 1])\n",
    "        label = label.transpose([2, 0, 1])\n",
    "\n",
    "        return (\n",
    "            torch.tensor(image, dtype=torch.float),\n",
    "            torch.tensor(label, dtype=torch.float)\n",
    "        )\n",
    "\n",
    "# Prepare the datasets.\n",
    "def get_datasets(\n",
    "    image_paths\n",
    "):\n",
    "    dataset_test = SRCNNDataset(image_paths)\n",
    "    return dataset_test\n",
    "\n",
    "# Prepare the data loaders\n",
    "def get_dataloaders(dataset_test):\n",
    "    test_loader = DataLoader(\n",
    "        dataset_test, \n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "    return test_loader\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the model.\n",
    "    model = SRCNN().to(device)\n",
    "    model.load_state_dict(torch.load('./data/srcnn_outputs/model.pth'))\n",
    "\n",
    "    data_paths = [\n",
    "        ['./data/raw/CGG_data/senti_test', 'Sentinel-2'],\n",
    "        ['./data/raw/CGG_data/ge_test', 'Google_earth']\n",
    "    ]\n",
    "\n",
    "    for data_path in data_paths:\n",
    "        dataset_test = get_datasets(data_path[0])\n",
    "        test_loader = get_dataloaders(dataset_test)\n",
    "\n",
    "        #_, test_psnr = validate(model, test_loader, device)\n",
    "        #print(f\"Test PSNR on {data_path[1]}: {test_psnr:.3f}\")\n",
    "\n",
    "        result(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
